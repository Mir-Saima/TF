{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import TF dataset for first-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import TF dataset\n",
    "X1=pd.read_csv('Prot-Bert/TF_Training_Embedding_ProtBert.csv', header=None).iloc[:,1:].values\n",
    "X2=pd.read_csv('Prot-Bert/NTF_Training_Embedding_ProtBert.csv', header=None).iloc[:,1:].values\n",
    "X_train = np.concatenate((X1,X2),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels = np.ones(413)\n",
    "neg_labels = np.zeros(416)\n",
    "y_train = np.concatenate((pos_labels,neg_labels),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=pd.read_csv('Prot-Bert/TF_Ind_Embedding_ProtBert.csv', header=None).iloc[:,1:].values\n",
    "X2=pd.read_csv('Prot-Bert/NTF_Ind_Embedding_ProtBert.csv', header=None).iloc[:,1:].values\n",
    "X_test = np.concatenate((X1,X2),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels = np.ones(106)\n",
    "neg_labels = np.zeros(106)\n",
    "y_test = np.concatenate((pos_labels,neg_labels),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=67, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_test(model, X_test, y_test):\n",
    "    from sklearn import metrics\n",
    "\n",
    "    # Predict Test Data \n",
    "    y_pred = model.predict_proba(X_test)[:,1]\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i]>0.5:\n",
    "            y_pred[i]=1\n",
    "        else:\n",
    "            y_pred[i]=0\n",
    "    \n",
    "\n",
    "    # Calculate accuracy, precision, recall, f1-score, and kappa score\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    prec = metrics.precision_score(y_test, y_pred)\n",
    "    rec = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate area under curve (AUC)\n",
    "    y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    #MCC\n",
    "    mcc=matthews_corrcoef(y_test, model.predict(X_test))\n",
    "    \n",
    "    # Display confussion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    total=sum(sum(cm))\n",
    "    \n",
    "    #accuracy=(cm[0,0]+cm[1,1])/total\n",
    "    spec = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "    sen= cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "    \n",
    "#     print(y_pred_proba)\n",
    "\n",
    "    return {'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'mcc':mcc,\n",
    "            'fpr': fpr, 'tpr': tpr, 'auc': auc, 'cm': cm, 'sen': sen, 'spec':spec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def evaluate_model_train(model, X_train, y_train):\n",
    "    conf_matrix_list_of_arrays = []\n",
    "    mcc_array=[]\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1) \n",
    "    lst_accu = []\n",
    "    AUC_list=[]\n",
    "    Sen_list=[]\n",
    "    Spec_list=[]\n",
    "    \n",
    "    score=cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy',n_jobs=-1, error_score='raise')\n",
    "    prec_train=np.mean(cross_val_score(model, X_train, y_train, cv=cv, scoring='precision'))\n",
    "    recall_train=np.mean(cross_val_score(model, X_train, y_train, cv=cv, scoring='recall'))\n",
    "    f1_train=np.mean(cross_val_score(model, X_train, y_train, cv=cv, scoring='f1'))\n",
    "    \n",
    "    \n",
    "    for train_index, test_index in cv.split(X_train, y_train): \n",
    "        X_train_fold, X_test_fold = X_train[train_index], X_train[test_index] \n",
    "        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index] \n",
    "        \n",
    "        model.fit(X_train_fold, y_train_fold) \n",
    "        #lst_accu.append(model.score(X_test_fold, y_test_fold))\n",
    "        \n",
    "        #CM\n",
    "        conf_matrix = confusion_matrix(y_test_fold, model.predict(X_test_fold))\n",
    "        conf_matrix_list_of_arrays.append(conf_matrix)\n",
    "        cm=conf_matrix\n",
    "#         #Spec\n",
    "#         spec = round(cm[0,0]/(cm[0,1]+cm[0,0]),3)\n",
    "#         Spec_list.append(spec)\n",
    "#         #Sen\n",
    "#         sen = round(cm[1,1]/(cm[1,0]+cm[1,1]),3)\n",
    "#         Sen_list.append(sen)\n",
    "        \n",
    "        \n",
    "        #MCC\n",
    "        MCC=matthews_corrcoef(y_test_fold, model.predict(X_test_fold))\n",
    "        mcc_array.append(round(MCC, 3))\n",
    "\n",
    "        \n",
    "        # Calculate area under curve (AUC)\n",
    "        AUC=metrics.roc_auc_score( y_test_fold, model.predict_proba(X_test_fold)[:,1])\n",
    "                         \n",
    "        AUC_list.append(round(AUC,3))\n",
    "        \n",
    "        \n",
    "    auc=np.mean(AUC_list)    \n",
    "    mcc=np.mean(mcc_array, axis=0)   \n",
    "    cm = np.mean(conf_matrix_list_of_arrays, axis=0)    \n",
    "    total=sum(sum(cm))\n",
    "    accuracy=(cm[0,0]+cm[1,1])/total\n",
    "    specificity = cm[0,0]/(cm[0,1]+cm[0,0])\n",
    "    sensitivity = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "    \n",
    "    \n",
    "    return {'prec_train': prec_train, 'recall_train': recall_train,\n",
    "            'f1_train': f1_train, 'cm': cm, 'mcc': mcc,'acc':accuracy,\n",
    "           'sen':sensitivity,'spec':specificity, 'auc':auc, 'score':score,\n",
    "            'mcc_list': mcc_array, 'auc_list':AUC_list, 'Sen_list':Sen_list, 'Spec_list':Spec_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=24) \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def RF_objective(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 60)\n",
    "    max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 2, 1000)\n",
    "    min_samples_split= trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    \n",
    "    ## Create Model\n",
    "    model = RandomForestClassifier(max_depth = max_depth, min_samples_split=min_samples_split,\n",
    "                                   n_estimators = n_estimators,n_jobs=2\n",
    "                                    )\n",
    "\n",
    "   \n",
    "    score = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "    return accuracy_mean\n",
    "\n",
    "#Execute optuna and set hyperparameters\n",
    "RF_study = optuna.create_study(direction='maximize')\n",
    "RF_study.optimize(RF_objective, n_trails=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_RF=RandomForestClassifier(**RF_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_RF, X_train, y_train)\n",
    "print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"The Acc value from CM is: \", train_eval['acc'])\n",
    "print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['auc'])\n",
    "print('5 accuracies: ', train_eval['score'])\n",
    "Acc_rf=train_eval['score']\n",
    "Sen_rf=train_eval['Sen_list']\n",
    "Spec_rf=train_eval['Spec_list']\n",
    "MCC_rf=train_eval['mcc_list']\n",
    "AUC_rf=train_eval['auc_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "#rfc.fit(X_train, y_train)\n",
    "dtc_eval = evaluate_model_test(optimized_RF, X_test, y_test)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import optuna\n",
    "def objective(trial):\n",
    "    \"\"\"Define the objective function\"\"\"\n",
    "    params = {\n",
    "            'n_estimators' : trial.suggest_int('n_estimators', 100, 2000),\n",
    "            'max_depth' : trial.suggest_int('max_depth', 10, 90),\n",
    "            'max_leaf_nodes' : trial.suggest_int('max_leaf_nodes', 15, 100),\n",
    "            'criterion' : trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    # Fit the model\n",
    "    etc_model = ExtraTreesClassifier(**params)\n",
    "    score = cross_val_score(etc_model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "    return accuracy_mean\n",
    "\n",
    "\n",
    "#Execute optuna and set hyperparameters\n",
    "etc_study = optuna.create_study(direction='maximize')\n",
    "etc_study.optimize(objective, n_trails=200)\n",
    "\n",
    "optimized_etc =ExtraTreesClassifier(**etc_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_etc, X_train, y_train)\n",
    "print(\"Confusion Matrix is:\\n\", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"Recall value is: \", train_eval['recall_train'])\n",
    "print('The area under curve is:', train_eval['auc'])\n",
    "print(\"F1 score is: \", train_eval['f1_train'])\n",
    "Acc_etc=train_eval['score']\n",
    "Sen_etc=train_eval['Sen_list']\n",
    "Spec_etc=train_eval['Spec_list']\n",
    "MCC_etc=train_eval['mcc_list']\n",
    "AUC_etc=train_eval['auc_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_etc, X_test, y_test)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "#cv = RepeatedStratifiedKFold(n_splits=5)\n",
    "import optuna\n",
    "def objective(trial):\n",
    "    \"\"\"Define the objective function\"\"\"\n",
    "\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 400),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 10.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 50),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 10.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.01, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.01, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0),\n",
    "        #'eval_metric': 'mlogloss',\n",
    "        #'use_label_encoder': False\n",
    "    }\n",
    "\n",
    "    # Fit the model\n",
    "    xgb_model = XGBClassifier(**params,  eval_metric='mlogloss')\n",
    "    score = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "    return accuracy_mean\n",
    "#Execute optuna and set hyperparameters\n",
    "XGB_study = optuna.create_study(direction='maximize')\n",
    "XGB_study.optimize(objective, n_trails=200)\n",
    "optimized_XGB =XGBClassifier(**XGB_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_XGB, X_train, y_train)\n",
    "\n",
    "print(\"Confusion Matrix is:\\n\", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"Recall value is: \", train_eval['recall_train'])\n",
    "print(\"F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['auc'])\n",
    "Acc_xgb=train_eval['score']\n",
    "Sen_xgb=train_eval['Sen_list']\n",
    "Spec_xgb=train_eval['Spec_list']\n",
    "MCC_xgb=train_eval['mcc_list']\n",
    "AUC_xgb=train_eval['auc_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_XGB, X_test, y_test)\n",
    "\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "def objective(trial):\n",
    "    \"\"\"Define the objective function\"\"\"\n",
    "    params = {\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 100), \n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 100), \n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 10), \n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000), \n",
    "        #'objective': 'multiclass', \n",
    "       # 'class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 100), \n",
    "        'subsample': trial.suggest_uniform('subsample', 0.7, 1.0), \n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.7, 1.0),\n",
    "        'reg_alpha': trial.suggest_uniform('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_uniform('reg_lambda', 0.0, 10.0),\n",
    "        'random_state': 0\n",
    "    }\n",
    "\n",
    "\n",
    "    # Fit the model\n",
    "    lgbm_model = lgbm.LGBMClassifier(**params)\n",
    "    score = cross_val_score(lgbm_model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "\n",
    "    return accuracy_mean\n",
    "\n",
    "\n",
    "#Execute optuna and set hyperparameters\n",
    "lgbm_study = optuna.create_study(direction='maximize')\n",
    "lgbm_study.optimize(objective, n_trails=200)\n",
    "\n",
    "optimized_lgbm =lgbm.LGBMClassifier(**lgbm_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_lgbm, X_train, y_train)\n",
    "print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"The Precision value is: \", train_eval['prec_train'])\n",
    "print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['auc'])\n",
    "Acc_lgbm=train_eval['score']\n",
    "Sen_lgbm=train_eval['Sen_list']\n",
    "Spec_lgbm=train_eval['Spec_list']\n",
    "MCC_lgbm=train_eval['mcc_list']\n",
    "AUC_lgbm=train_eval['auc_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_lgbm, X_test, y_test)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# def objective(trial):\n",
    "    \n",
    "#     params = {\n",
    "#                 'n_estimators':trial.suggest_int('n_estimators',50,500),\n",
    "#                 'learning_rate': trial.suggest_float('learning_rate', 0.1,2.5,step = 0.0000005),\n",
    "#                 'algorithm':'SAMME.R', \n",
    "#                 'random_state':47\n",
    "#             }\n",
    "    \n",
    "    \n",
    "#     # Fit the model\n",
    "#     abc_model = AdaBoostClassifier(**params)\n",
    "    \n",
    "#     score = cross_val_score(abc_model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "#     accuracy_mean = score.mean()\n",
    "\n",
    "#     return accuracy_mean\n",
    "\n",
    "\n",
    "# #Execute optuna and set hyperparameters\n",
    "# abc_study = optuna.create_study(direction='maximize')\n",
    "# abc_study.optimize(objective, n_trails=200)\n",
    "\n",
    "# optimized_abc =AdaBoostClassifier(**abc_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate Model on Training data\n",
    "# train_eval = evaluate_model_train(optimized_abc, X_train, y_train)\n",
    "# print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "# print ('Accuracy : ', train_eval['acc'])\n",
    "# print('Sensitivity : ', train_eval['sen'])\n",
    "# print('Specificity : ', train_eval['spec'])\n",
    "# print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "# print(\"The Precision value is: \", train_eval['prec_train'])\n",
    "# print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "# print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "# print('The area under curve is:', train_eval['auc'])\n",
    "# AAindex_abc=train_eval['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate Model on Testing data\n",
    "# dtc_eval = evaluate_model_test(optimized_abc, X_test, y_test)\n",
    "# # Print result\n",
    "# print('Accuracy:', dtc_eval['acc'])\n",
    "# print('Precision:', dtc_eval['prec'])\n",
    "# print('Recall:', dtc_eval['rec'])\n",
    "# print('F1 Score:', dtc_eval['f1'])\n",
    "# print('Area Under Curve:', dtc_eval['auc'])\n",
    "# print('Sensitivity : ', dtc_eval['sen'])\n",
    "# print('Specificity : ', dtc_eval['spec'])\n",
    "# print('MCC Score : ', dtc_eval['mcc'])\n",
    "# print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#             \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.5),\n",
    "#             \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
    "#             \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
    "#             \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "#             \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "#             \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
    "#             ),\n",
    "#             \"used_ram_limit\": \"3gb\",\n",
    "#         }\n",
    "\n",
    "# #     if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "# #         param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "# #     elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "# #         param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
    "\n",
    "\n",
    "#     # Fit the model\n",
    "#     cat_model = CatBoostClassifier(**params)\n",
    "\n",
    "#     score = cross_val_score(cat_model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "#     accuracy_mean = score.mean()\n",
    "\n",
    "#     return accuracy_mean\n",
    "\n",
    "\n",
    "# #Execute optuna and set hyperparameters\n",
    "# cat_study = optuna.create_study(direction='maximize')\n",
    "# cat_study.optimize(objective, n_trials=5)\n",
    "\n",
    "# optimized_cat =CatBoostClassifier(**cat_study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate Model on Training data\n",
    "# train_eval = evaluate_model_train(optimized_cat, X_train, y_train)\n",
    "# print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "# print ('Accuracy : ', train_eval['acc'])\n",
    "# print('Sensitivity : ', train_eval['sen'])\n",
    "# print('Specificity : ', train_eval['spec'])\n",
    "# print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "# print(\"The Precision value is: \", train_eval['prec_train'])\n",
    "# print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "# print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "# print('The area under curve is:', train_eval['auc'])\n",
    "# cbc=train_eval['score']\n",
    "# Sen_cbc=train_eval['Sen_list']\n",
    "# Spec_cbc=train_eval['Spec_list']\n",
    "# MCC_cbc=train_eval['mcc_list']\n",
    "# AUC_cbc=train_eval['auc_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate Model on Testing data\n",
    "# dtc_eval = evaluate_model_test(optimized_cat, X_test, y_test)\n",
    "# # Print result\n",
    "# print('Accuracy:', dtc_eval['acc'])\n",
    "# print('Precision:', dtc_eval['prec'])\n",
    "# print('Recall:', dtc_eval['rec'])\n",
    "# print('F1 Score:', dtc_eval['f1'])\n",
    "# print('Area Under Curve:', dtc_eval['auc'])\n",
    "# print('Sensitivity : ', dtc_eval['sen'])\n",
    "# print('Specificity : ', dtc_eval['spec'])\n",
    "# print('MCC Score : ', dtc_eval['mcc'])\n",
    "# print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# box= pd.DataFrame({1:rf, 2:xgb, 3:etc, 4:lgbm, 5:cbc})\n",
    "# # boxplot=sns.boxplot(data=box_AAindex, width=0.5)\n",
    "# # boxplot.set_xlabel(\"AAindex\", fontsize=14)\n",
    "# # boxplot.set_ylabel(\"Accuracy\", fontsize=14)\n",
    "# # plt.show()\n",
    "# box=pd.DataFrame(box)\n",
    "# box.to_csv('Box_SCPAAC_Accuracies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for Optuna\n",
    "from sklearn.svm import SVC\n",
    "def objective(trial):\n",
    "    # C\n",
    "    svc_c = trial.suggest_float('C', 1e0, 1e2)\n",
    "    # kernel\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf'])\n",
    "    # SVC\n",
    "    clf = SVC(C=svc_c, kernel=kernel)\n",
    "    score = cross_val_score(clf, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "\n",
    "    return accuracy_mean\n",
    "\n",
    "\n",
    "#Execute optuna and set hyperparameters\n",
    "svm_study = optuna.create_study(direction='maximize')\n",
    "svm_study.optimize(objective, n_trails=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_svm =SVC(**svm_study.best_params, probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_svm, X_train, y_train)\n",
    "print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"The Precision value is: \", train_eval['prec_train'])\n",
    "print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['auc'])\n",
    "Acc_svm=train_eval['score']\n",
    "Sen_svm=train_eval['Sen_list']\n",
    "Spec_svm=train_eval['Spec_list']\n",
    "MCC_svm=train_eval['mcc_list']\n",
    "AUC_svm=train_eval['auc_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_svm, X_test, y_test)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "\n",
    "# box_ACC= pd.DataFrame({1:Acc_rf, 2:Acc_xgb, 3:Acc_etc, 4:Acc_lgbm, 5:Acc_svm})\n",
    "# box_Sen= pd.DataFrame({1:Sen_rf, 2:Sen_xgb, 3:Sen_etc, 4:Sen_lgbm, 5:Sen_svm})\n",
    "# box_Spec= pd.DataFrame({1:Spec_rf, 2:Spec_xgb, 3:Spec_etc, 4:Spec_lgbm, 5:Spec_svm})\n",
    "# box_MCC= pd.DataFrame({1:MCC_rf, 2:MCC_xgb, 3:MCC_etc, 4:MCC_lgbm, 5:MCC_svm})\n",
    "# box_AUC= pd.DataFrame({1:AUC_rf, 2:AUC_xgb, 3:AUC_etc, 4:AUC_lgbm, 5:AUC_svm})\n",
    "\n",
    "# boxplot=sns.boxplot(data=box_ACC, width=0.5)\n",
    "# boxplot.set_xlabel(\"PAAC\", fontsize=14)\n",
    "# boxplot.set_ylabel(\"Accuracy\", fontsize=14)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Saving the models\n",
    "# import pickle\n",
    "# pickle.dump(optimized_RF, open('Models/Optimized_RF_PAAC.pkl', 'wb'))\n",
    "# pickle.dump(optimized_XGB, open('Models/Optimized_XGB_PAAC.pkl', 'wb'))\n",
    "# pickle.dump(optimized_etc, open('Models/Optimized_etc_PAAC.pkl', 'wb'))\n",
    "# pickle.dump(optimized_lgbm, open('Models/Optimized_lgbm_PAAC.pkl', 'wb'))\n",
    "# pickle.dump(optimized_svm, open('Models/Optimized_SVM_PAAC.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import TFPM dataset for second-layer model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=pd.read_csv('Prot-Bert/TFPM/TFPM_Training_Embedding_ProtBert.csv', header=None).iloc[:,1:].values\n",
    "X2=pd.read_csv('Prot-Bert/TFPM/TFPNM_Training_Embedding_ProtBert.csv', header=None).iloc[:,1:].values\n",
    "X_train2 = np.concatenate((X1,X2),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels = np.ones(146)\n",
    "neg_labels = np.zeros(146)\n",
    "y_train2 = np.concatenate((pos_labels,neg_labels),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=pd.read_csv('Prot-Bert/TFPM/TFPM_Ind_Embedding_ProtBert.csv', header=None).iloc[:,1:].values\n",
    "X2=pd.read_csv('Prot-Bert/TFPM/TFPNM_Ind_Embedding_ProtBert.csv', header=None).iloc[:,1:].values\n",
    "X_test2 = np.concatenate((X1,X2),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels = np.ones(69)\n",
    "neg_labels = np.zeros(37)\n",
    "y_test2 = np.concatenate((pos_labels,neg_labels),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=67, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2.shape, X_test2.shape, y_train2.shape, y_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1=pd.read_csv('Prot-Bert/TFPM/Features/Binding_Training_PAAC.csv', header=None).iloc[:,1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_labels = np.ones(146)\n",
    "# neg_labels = np.zeros(146)\n",
    "# y1 = np.concatenate((pos_labels,neg_labels),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1.shape, y1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=24) \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def RF_objective(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 60)\n",
    "    max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 2, 1000)\n",
    "    min_samples_split= trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    \n",
    "    ## Create Model\n",
    "    model = RandomForestClassifier(max_depth = max_depth, min_samples_split=min_samples_split,\n",
    "                                   n_estimators = n_estimators,n_jobs=2\n",
    "                                    )\n",
    "\n",
    "   \n",
    "    score = cross_val_score(model, X_train2, y_train2, cv=cv, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "    return accuracy_mean\n",
    "\n",
    "#Execute optuna and set hyperparameters\n",
    "RF_study = optuna.create_study(direction='maximize')\n",
    "RF_study.optimize(RF_objective, n_trails=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_RF=RandomForestClassifier(**RF_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_RF, X_train2, y_train2)\n",
    "print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"The Acc value from CM is: \", train_eval['acc'])\n",
    "print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['auc'])\n",
    "print('5 accuracies: ', train_eval['score'])\n",
    "Acc_rf=train_eval['score']\n",
    "Sen_rf=train_eval['Sen_list']\n",
    "Spec_rf=train_eval['Spec_list']\n",
    "MCC_rf=train_eval['mcc_list']\n",
    "AUC_rf=train_eval['auc_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "#rfc.fit(X_train, y_train)\n",
    "dtc_eval = evaluate_model_test(optimized_RF, X_test2, y_test2)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import optuna\n",
    "def objective(trial):\n",
    "    \"\"\"Define the objective function\"\"\"\n",
    "    params = {\n",
    "            'n_estimators' : trial.suggest_int('n_estimators', 100, 2000),\n",
    "            'max_depth' : trial.suggest_int('max_depth', 10, 90),\n",
    "            'max_leaf_nodes' : trial.suggest_int('max_leaf_nodes', 15, 100),\n",
    "            'criterion' : trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    # Fit the model\n",
    "    etc_model = ExtraTreesClassifier(**params)\n",
    "    score = cross_val_score(etc_model, X_train2, y_train2, cv=cv, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "    return accuracy_mean\n",
    "\n",
    "\n",
    "#Execute optuna and set hyperparameters\n",
    "etc_study = optuna.create_study(direction='maximize')\n",
    "etc_study.optimize(objective, n_trails=200)\n",
    "\n",
    "optimized_etc =ExtraTreesClassifier(**etc_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_etc, X_train2, y_train2)\n",
    "print(\"Confusion Matrix is:\\n\", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"Recall value is: \", train_eval['recall_train'])\n",
    "print('The area under curve is:', train_eval['auc'])\n",
    "print(\"F1 score is: \", train_eval['f1_train'])\n",
    "Acc_etc=train_eval['score']\n",
    "Sen_etc=train_eval['Sen_list']\n",
    "Spec_etc=train_eval['Spec_list']\n",
    "MCC_etc=train_eval['mcc_list']\n",
    "AUC_etc=train_eval['auc_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_etc, X_test2, y_test2)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "#cv = RepeatedStratifiedKFold(n_splits=5)\n",
    "import optuna\n",
    "def objective(trial):\n",
    "    \"\"\"Define the objective function\"\"\"\n",
    "\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 400),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 10.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 50),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 10.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.01, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.01, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0),\n",
    "        #'eval_metric': 'mlogloss',\n",
    "        #'use_label_encoder': False\n",
    "    }\n",
    "\n",
    "    # Fit the model\n",
    "    xgb_model = XGBClassifier(**params,  eval_metric='mlogloss')\n",
    "    score = cross_val_score(xgb_model, X_train2, y_train2, cv=cv, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "    return accuracy_mean\n",
    "\n",
    "\n",
    "#Execute optuna and set hyperparameters\n",
    "XGB_study = optuna.create_study(direction='maximize')\n",
    "XGB_study.optimize(objective, n_trails=200)\n",
    "optimized_XGB =XGBClassifier(**XGB_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_XGB, X_train2, y_train2)\n",
    "\n",
    "print(\"Confusion Matrix is:\\n\", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"Recall value is: \", train_eval['recall_train'])\n",
    "print(\"F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['auc'])\n",
    "Acc_xgb=train_eval['score']\n",
    "Sen_xgb=train_eval['Sen_list']\n",
    "Spec_xgb=train_eval['Spec_list']\n",
    "MCC_xgb=train_eval['mcc_list']\n",
    "AUC_xgb=train_eval['auc_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_XGB, X_test2, y_test2)\n",
    "\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "def objective(trial):\n",
    "    \"\"\"Define the objective function\"\"\"\n",
    "    params = {\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 100), \n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 100), \n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 10), \n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000), \n",
    "        #'objective': 'multiclass', \n",
    "       # 'class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 100), \n",
    "        'subsample': trial.suggest_uniform('subsample', 0.7, 1.0), \n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.7, 1.0),\n",
    "        'reg_alpha': trial.suggest_uniform('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_uniform('reg_lambda', 0.0, 10.0),\n",
    "        'random_state': 0\n",
    "    }\n",
    "\n",
    "\n",
    "    # Fit the model\n",
    "    lgbm_model = lgbm.LGBMClassifier(**params)\n",
    "    score = cross_val_score(lgbm_model, X_train2, y_train2, cv=cv, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "\n",
    "    return accuracy_mean\n",
    "\n",
    "\n",
    "#Execute optuna and set hyperparameters\n",
    "lgbm_study = optuna.create_study(direction='maximize')\n",
    "lgbm_study.optimize(objective, n_trails=200)\n",
    "\n",
    "optimized_lgbm =lgbm.LGBMClassifier(**lgbm_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_lgbm, X_train2, y_train2)\n",
    "print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"The Precision value is: \", train_eval['prec_train'])\n",
    "print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['auc'])\n",
    "Acc_lgbm=train_eval['score']\n",
    "Sen_lgbm=train_eval['Sen_list']\n",
    "Spec_lgbm=train_eval['Spec_list']\n",
    "MCC_lgbm=train_eval['mcc_list']\n",
    "AUC_lgbm=train_eval['auc_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_lgbm, X_test2, y_test2)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# def objective(trial):\n",
    "    \n",
    "#     params = {\n",
    "#                 'n_estimators':trial.suggest_int('n_estimators',50,500),\n",
    "#                 'learning_rate': trial.suggest_float('learning_rate', 0.1,2.5,step = 0.0000005),\n",
    "#                 'algorithm':'SAMME.R', \n",
    "#                 'random_state':47\n",
    "#             }\n",
    "    \n",
    "    \n",
    "#     # Fit the model\n",
    "#     abc_model = AdaBoostClassifier(**params)\n",
    "    \n",
    "#     score = cross_val_score(abc_model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "#     accuracy_mean = score.mean()\n",
    "\n",
    "#     return accuracy_mean\n",
    "\n",
    "\n",
    "# #Execute optuna and set hyperparameters\n",
    "# abc_study = optuna.create_study(direction='maximize')\n",
    "# abc_study.optimize(objective, n_trails=200)\n",
    "\n",
    "# optimized_abc =AdaBoostClassifier(**abc_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate Model on Training data\n",
    "# train_eval = evaluate_model_train(optimized_abc, X_train, y_train)\n",
    "# print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "# print ('Accuracy : ', train_eval['acc'])\n",
    "# print('Sensitivity : ', train_eval['sen'])\n",
    "# print('Specificity : ', train_eval['spec'])\n",
    "# print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "# print(\"The Precision value is: \", train_eval['prec_train'])\n",
    "# print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "# print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "# print('The area under curve is:', train_eval['auc'])\n",
    "# AAindex_abc=train_eval['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate Model on Testing data\n",
    "# dtc_eval = evaluate_model_test(optimized_abc, X_test, y_test)\n",
    "# # Print result\n",
    "# print('Accuracy:', dtc_eval['acc'])\n",
    "# print('Precision:', dtc_eval['prec'])\n",
    "# print('Recall:', dtc_eval['rec'])\n",
    "# print('F1 Score:', dtc_eval['f1'])\n",
    "# print('Area Under Curve:', dtc_eval['auc'])\n",
    "# print('Sensitivity : ', dtc_eval['sen'])\n",
    "# print('Specificity : ', dtc_eval['spec'])\n",
    "# print('MCC Score : ', dtc_eval['mcc'])\n",
    "# print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#             \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.5),\n",
    "#             \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
    "#             \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
    "#             \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "#             \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "#             \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
    "#             ),\n",
    "#             \"used_ram_limit\": \"3gb\",\n",
    "#         }\n",
    "\n",
    "# #     if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "# #         param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "# #     elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "# #         param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
    "\n",
    "\n",
    "#     # Fit the model\n",
    "#     cat_model = CatBoostClassifier(**params)\n",
    "\n",
    "#     score = cross_val_score(cat_model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "#     accuracy_mean = score.mean()\n",
    "\n",
    "#     return accuracy_mean\n",
    "\n",
    "\n",
    "# #Execute optuna and set hyperparameters\n",
    "# cat_study = optuna.create_study(direction='maximize')\n",
    "# cat_study.optimize(objective, n_trials=5)\n",
    "\n",
    "# optimized_cat =CatBoostClassifier(**cat_study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate Model on Training data\n",
    "# train_eval = evaluate_model_train(optimized_cat, X_train, y_train)\n",
    "# print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "# print ('Accuracy : ', train_eval['acc'])\n",
    "# print('Sensitivity : ', train_eval['sen'])\n",
    "# print('Specificity : ', train_eval['spec'])\n",
    "# print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "# print(\"The Precision value is: \", train_eval['prec_train'])\n",
    "# print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "# print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "# print('The area under curve is:', train_eval['auc'])\n",
    "# cbc=train_eval['score']\n",
    "# Sen_cbc=train_eval['Sen_list']\n",
    "# Spec_cbc=train_eval['Spec_list']\n",
    "# MCC_cbc=train_eval['mcc_list']\n",
    "# AUC_cbc=train_eval['auc_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate Model on Testing data\n",
    "# dtc_eval = evaluate_model_test(optimized_cat, X_test, y_test)\n",
    "# # Print result\n",
    "# print('Accuracy:', dtc_eval['acc'])\n",
    "# print('Precision:', dtc_eval['prec'])\n",
    "# print('Recall:', dtc_eval['rec'])\n",
    "# print('F1 Score:', dtc_eval['f1'])\n",
    "# print('Area Under Curve:', dtc_eval['auc'])\n",
    "# print('Sensitivity : ', dtc_eval['sen'])\n",
    "# print('Specificity : ', dtc_eval['spec'])\n",
    "# print('MCC Score : ', dtc_eval['mcc'])\n",
    "# print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# box= pd.DataFrame({1:rf, 2:xgb, 3:etc, 4:lgbm, 5:cbc})\n",
    "# # boxplot=sns.boxplot(data=box_AAindex, width=0.5)\n",
    "# # boxplot.set_xlabel(\"AAindex\", fontsize=14)\n",
    "# # boxplot.set_ylabel(\"Accuracy\", fontsize=14)\n",
    "# # plt.show()\n",
    "# box=pd.DataFrame(box)\n",
    "# box.to_csv('Box_SCPAAC_Accuracies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Optuna\n",
    "from sklearn.svm import SVC\n",
    "def objective(trial):\n",
    "    # C\n",
    "    svc_c = trial.suggest_float('C', 1e0, 1e2)\n",
    "    # kernel\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf'])\n",
    "    # SVC\n",
    "    clf = SVC(C=svc_c, kernel=kernel)\n",
    "    score = cross_val_score(clf, X_train2, y_train2, cv=cv, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "\n",
    "    return accuracy_mean\n",
    "\n",
    "\n",
    "#Execute optuna and set hyperparameters\n",
    "svm_study = optuna.create_study(direction='maximize')\n",
    "svm_study.optimize(objective, n_trails=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_svm =SVC(**svm_study.best_params, probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_svm, X_train2, y_train2)\n",
    "print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"The Precision value is: \", train_eval['prec_train'])\n",
    "print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['auc'])\n",
    "Acc_svm=train_eval['score']\n",
    "Sen_svm=train_eval['Sen_list']\n",
    "Spec_svm=train_eval['Spec_list']\n",
    "MCC_svm=train_eval['mcc_list']\n",
    "AUC_svm=train_eval['auc_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_svm, X_test2, y_test2)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Unbalanced dataset to check the generalizability of the TFProtBert method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the NTF_samples in the train and test part.\n",
    "X=pd.read_csv('Prot-Bert/NTF_6444_Embedding_ProtBert.csv', header=None).iloc[:,1:].values\n",
    "y = np.zeros(6444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_NTF_train, X_NTF_test, y_NTF_train, y_NTF_test= train_test_split(X, y, test_size=0.2, random_state=67, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_NTF_train.shape, X_NTF_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos=pd.read_csv('Prot-Bert/TF_Training_Embedding_ProtBert.csv', header=None).iloc[:,1:].values\n",
    "test_pos=pd.read_csv('Prot-Bert/TF_Ind_Embedding_ProtBert.csv', header=None).iloc[:,1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join the TF train and NTF train\n",
    "#Also join TF Ind and NTF Ind\n",
    "X_train=np.concatenate((train_pos, X_NTF_train), axis=0) \n",
    "X_test=np.concatenate((test_pos, X_NTF_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos_train=np.ones(413)\n",
    "y_train=np.concatenate((y_pos_train, y_NTF_train), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos_test=np.ones(106)\n",
    "y_test=np.concatenate((y_pos_test, y_NTF_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape\n",
    "\n",
    "y_train.shape, y_test.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(y_train), Counter(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=24) \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def RF_objective(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 60)\n",
    "    max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 2, 1000)\n",
    "    min_samples_split= trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    \n",
    "    ## Create Model\n",
    "    model = RandomForestClassifier(max_depth = max_depth, min_samples_split=min_samples_split,\n",
    "                                   n_estimators = n_estimators,n_jobs=2\n",
    "                                    )\n",
    "\n",
    "   \n",
    "    score = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "    return accuracy_mean\n",
    "\n",
    "#Execute optuna and set hyperparameters\n",
    "RF_study = optuna.create_study(direction='maximize')\n",
    "RF_study.optimize(RF_objective, n_trails=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'n_estimators': 209, 'max_depth': 10, 'max_leaf_nodes': 60, 'min_samples_split': 2}\n",
    "optimized_RF=RandomForestClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_RF, X_train, y_train)\n",
    "print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"The Acc value from CM is: \", train_eval['acc'])\n",
    "print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['auc'])\n",
    "print('5 accuracies: ', train_eval['score'])\n",
    "Acc_rf=train_eval['score']\n",
    "Sen_rf=train_eval['Sen_list']\n",
    "Spec_rf=train_eval['Spec_list']\n",
    "MCC_rf=train_eval['mcc_list']\n",
    "AUC_rf=train_eval['auc_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "#rfc.fit(X_train, y_train)\n",
    "dtc_eval = evaluate_model_test(optimized_RF, X_test, y_test)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import optuna\n",
    "def objective(trial):\n",
    "    \"\"\"Define the objective function\"\"\"\n",
    "    params = {\n",
    "            'n_estimators' : trial.suggest_int('n_estimators', 100, 2000),\n",
    "            'max_depth' : trial.suggest_int('max_depth', 10, 90),\n",
    "            'max_leaf_nodes' : trial.suggest_int('max_leaf_nodes', 15, 100),\n",
    "            'criterion' : trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    # Fit the model\n",
    "    etc_model = ExtraTreesClassifier(**params)\n",
    "    score = cross_val_score(etc_model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "    return accuracy_mean\n",
    "\n",
    "\n",
    "#Execute optuna and set hyperparameters\n",
    "etc_study = optuna.create_study(direction='maximize')\n",
    "etc_study.optimize(objective, n_trails=200)\n",
    "\n",
    "optimized_etc =ExtraTreesClassifier(**etc_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_etc, X_train, y_train)\n",
    "print(\"Confusion Matrix is:\\n\", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"Recall value is: \", train_eval['recall_train'])\n",
    "print('The area under curve is:', train_eval['auc'])\n",
    "print(\"F1 score is: \", train_eval['f1_train'])\n",
    "Acc_etc=train_eval['score']\n",
    "Sen_etc=train_eval['Sen_list']\n",
    "Spec_etc=train_eval['Spec_list']\n",
    "MCC_etc=train_eval['mcc_list']\n",
    "AUC_etc=train_eval['auc_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_etc, X_test, y_test)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "#cv = RepeatedStratifiedKFold(n_splits=5)\n",
    "import optuna\n",
    "def objective(trial):\n",
    "    \"\"\"Define the objective function\"\"\"\n",
    "\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 400),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 10.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 50),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 10.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.01, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.01, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0),\n",
    "        #'eval_metric': 'mlogloss',\n",
    "        #'use_label_encoder': False\n",
    "    }\n",
    "\n",
    "    # Fit the model\n",
    "    xgb_model = XGBClassifier(**params,  eval_metric='mlogloss')\n",
    "    score = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "    return accuracy_mean\n",
    "#Execute optuna and set hyperparameters\n",
    "XGB_study = optuna.create_study(direction='maximize')\n",
    "XGB_study.optimize(objective, n_trails=200)\n",
    "optimized_XGB =XGBClassifier(**XGB_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_XGB, X_train, y_train)\n",
    "\n",
    "print(\"Confusion Matrix is:\\n\", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"Recall value is: \", train_eval['recall_train'])\n",
    "print(\"F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['auc'])\n",
    "Acc_xgb=train_eval['score']\n",
    "Sen_xgb=train_eval['Sen_list']\n",
    "Spec_xgb=train_eval['Spec_list']\n",
    "MCC_xgb=train_eval['mcc_list']\n",
    "AUC_xgb=train_eval['auc_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_XGB, X_test, y_test)\n",
    "\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "def objective(trial):\n",
    "    \"\"\"Define the objective function\"\"\"\n",
    "    params = {\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 100), \n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 100), \n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 10), \n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000), \n",
    "        #'objective': 'multiclass', \n",
    "       # 'class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 100), \n",
    "        'subsample': trial.suggest_uniform('subsample', 0.7, 1.0), \n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.7, 1.0),\n",
    "        'reg_alpha': trial.suggest_uniform('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_uniform('reg_lambda', 0.0, 10.0),\n",
    "        'random_state': 0\n",
    "    }\n",
    "\n",
    "\n",
    "    # Fit the model\n",
    "    lgbm_model = lgbm.LGBMClassifier(**params)\n",
    "    score = cross_val_score(lgbm_model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "\n",
    "    return accuracy_mean\n",
    "\n",
    "\n",
    "#Execute optuna and set hyperparameters\n",
    "lgbm_study = optuna.create_study(direction='maximize')\n",
    "lgbm_study.optimize(objective, n_trails=200)\n",
    "\n",
    "optimized_lgbm =lgbm.LGBMClassifier(**lgbm_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_lgbm, X_train, y_train)\n",
    "print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"The Precision value is: \", train_eval['prec_train'])\n",
    "print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['auc'])\n",
    "Acc_lgbm=train_eval['score']\n",
    "Sen_lgbm=train_eval['Sen_list']\n",
    "Spec_lgbm=train_eval['Spec_list']\n",
    "MCC_lgbm=train_eval['mcc_list']\n",
    "AUC_lgbm=train_eval['auc_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_lgbm, X_test, y_test)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# def objective(trial):\n",
    "    \n",
    "#     params = {\n",
    "#                 'n_estimators':trial.suggest_int('n_estimators',50,500),\n",
    "#                 'learning_rate': trial.suggest_float('learning_rate', 0.1,2.5,step = 0.0000005),\n",
    "#                 'algorithm':'SAMME.R', \n",
    "#                 'random_state':47\n",
    "#             }\n",
    "    \n",
    "    \n",
    "#     # Fit the model\n",
    "#     abc_model = AdaBoostClassifier(**params)\n",
    "    \n",
    "#     score = cross_val_score(abc_model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "#     accuracy_mean = score.mean()\n",
    "\n",
    "#     return accuracy_mean\n",
    "\n",
    "\n",
    "# #Execute optuna and set hyperparameters\n",
    "# abc_study = optuna.create_study(direction='maximize')\n",
    "# abc_study.optimize(objective, n_trails=200)\n",
    "\n",
    "# optimized_abc =AdaBoostClassifier(**abc_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate Model on Training data\n",
    "# train_eval = evaluate_model_train(optimized_abc, X_train, y_train)\n",
    "# print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "# print ('Accuracy : ', train_eval['acc'])\n",
    "# print('Sensitivity : ', train_eval['sen'])\n",
    "# print('Specificity : ', train_eval['spec'])\n",
    "# print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "# print(\"The Precision value is: \", train_eval['prec_train'])\n",
    "# print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "# print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "# print('The area under curve is:', train_eval['auc'])\n",
    "# AAindex_abc=train_eval['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate Model on Testing data\n",
    "# dtc_eval = evaluate_model_test(optimized_abc, X_test, y_test)\n",
    "# # Print result\n",
    "# print('Accuracy:', dtc_eval['acc'])\n",
    "# print('Precision:', dtc_eval['prec'])\n",
    "# print('Recall:', dtc_eval['rec'])\n",
    "# print('F1 Score:', dtc_eval['f1'])\n",
    "# print('Area Under Curve:', dtc_eval['auc'])\n",
    "# print('Sensitivity : ', dtc_eval['sen'])\n",
    "# print('Specificity : ', dtc_eval['spec'])\n",
    "# print('MCC Score : ', dtc_eval['mcc'])\n",
    "# print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#             \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.5),\n",
    "#             \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
    "#             \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
    "#             \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "#             \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "#             \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
    "#             ),\n",
    "#             \"used_ram_limit\": \"3gb\",\n",
    "#         }\n",
    "\n",
    "# #     if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "# #         param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "# #     elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "# #         param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
    "\n",
    "\n",
    "#     # Fit the model\n",
    "#     cat_model = CatBoostClassifier(**params)\n",
    "\n",
    "#     score = cross_val_score(cat_model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "#     accuracy_mean = score.mean()\n",
    "\n",
    "#     return accuracy_mean\n",
    "\n",
    "\n",
    "# #Execute optuna and set hyperparameters\n",
    "# cat_study = optuna.create_study(direction='maximize')\n",
    "# cat_study.optimize(objective, n_trials=5)\n",
    "\n",
    "# optimized_cat =CatBoostClassifier(**cat_study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate Model on Training data\n",
    "# train_eval = evaluate_model_train(optimized_cat, X_train, y_train)\n",
    "# print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "# print ('Accuracy : ', train_eval['acc'])\n",
    "# print('Sensitivity : ', train_eval['sen'])\n",
    "# print('Specificity : ', train_eval['spec'])\n",
    "# print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "# print(\"The Precision value is: \", train_eval['prec_train'])\n",
    "# print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "# print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "# print('The area under curve is:', train_eval['auc'])\n",
    "# cbc=train_eval['score']\n",
    "# Sen_cbc=train_eval['Sen_list']\n",
    "# Spec_cbc=train_eval['Spec_list']\n",
    "# MCC_cbc=train_eval['mcc_list']\n",
    "# AUC_cbc=train_eval['auc_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate Model on Testing data\n",
    "# dtc_eval = evaluate_model_test(optimized_cat, X_test, y_test)\n",
    "# # Print result\n",
    "# print('Accuracy:', dtc_eval['acc'])\n",
    "# print('Precision:', dtc_eval['prec'])\n",
    "# print('Recall:', dtc_eval['rec'])\n",
    "# print('F1 Score:', dtc_eval['f1'])\n",
    "# print('Area Under Curve:', dtc_eval['auc'])\n",
    "# print('Sensitivity : ', dtc_eval['sen'])\n",
    "# print('Specificity : ', dtc_eval['spec'])\n",
    "# print('MCC Score : ', dtc_eval['mcc'])\n",
    "# print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# box= pd.DataFrame({1:rf, 2:xgb, 3:etc, 4:lgbm, 5:cbc})\n",
    "# # boxplot=sns.boxplot(data=box_AAindex, width=0.5)\n",
    "# # boxplot.set_xlabel(\"AAindex\", fontsize=14)\n",
    "# # boxplot.set_ylabel(\"Accuracy\", fontsize=14)\n",
    "# # plt.show()\n",
    "# box=pd.DataFrame(box)\n",
    "# box.to_csv('Box_SCPAAC_Accuracies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Optuna\n",
    "from sklearn.svm import SVC\n",
    "def objective(trial):\n",
    "    # C\n",
    "    svc_c = trial.suggest_float('C', 1e0, 1e2)\n",
    "    # kernel\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf'])\n",
    "    # SVC\n",
    "    clf = SVC(C=svc_c, kernel=kernel)\n",
    "    score = cross_val_score(clf, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "\n",
    "    return accuracy_mean\n",
    "\n",
    "\n",
    "#Execute optuna and set hyperparameters\n",
    "svm_study = optuna.create_study(direction='maximize')\n",
    "svm_study.optimize(objective, n_trails=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_svm =SVC(**svm_study.best_params, probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_svm, X_train, y_train)\n",
    "print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"The Precision value is: \", train_eval['prec_train'])\n",
    "print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['auc'])\n",
    "Acc_svm=train_eval['score']\n",
    "Sen_svm=train_eval['Sen_list']\n",
    "Spec_svm=train_eval['Spec_list']\n",
    "MCC_svm=train_eval['mcc_list']\n",
    "AUC_svm=train_eval['auc_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_svm, X_test, y_test)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "\n",
    "# box_ACC= pd.DataFrame({1:Acc_rf, 2:Acc_xgb, 3:Acc_etc, 4:Acc_lgbm, 5:Acc_svm})\n",
    "# box_Sen= pd.DataFrame({1:Sen_rf, 2:Sen_xgb, 3:Sen_etc, 4:Sen_lgbm, 5:Sen_svm})\n",
    "# box_Spec= pd.DataFrame({1:Spec_rf, 2:Spec_xgb, 3:Spec_etc, 4:Spec_lgbm, 5:Spec_svm})\n",
    "# box_MCC= pd.DataFrame({1:MCC_rf, 2:MCC_xgb, 3:MCC_etc, 4:MCC_lgbm, 5:MCC_svm})\n",
    "# box_AUC= pd.DataFrame({1:AUC_rf, 2:AUC_xgb, 3:AUC_etc, 4:AUC_lgbm, 5:AUC_svm})\n",
    "\n",
    "# boxplot=sns.boxplot(data=box_ACC, width=0.5)\n",
    "# boxplot.set_xlabel(\"PAAC\", fontsize=14)\n",
    "# boxplot.set_ylabel(\"Accuracy\", fontsize=14)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Saving the models\n",
    "# import pickle\n",
    "# pickle.dump(optimized_RF, open('Models/Optimized_RF_PAAC.pkl', 'wb'))\n",
    "# pickle.dump(optimized_XGB, open('Models/Optimized_XGB_PAAC.pkl', 'wb'))\n",
    "# pickle.dump(optimized_etc, open('Models/Optimized_etc_PAAC.pkl', 'wb'))\n",
    "# pickle.dump(optimized_lgbm, open('Models/Optimized_lgbm_PAAC.pkl', 'wb'))\n",
    "# pickle.dump(optimized_svm, open('Models/Optimized_SVM_PAAC.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
